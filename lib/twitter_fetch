import os
import requests
import pandas as pd

# API credentials
API_KEY = 'CI5Ekn6MBnN8s2q7VLFushZXx'
API_SECRET_KEY = '2lRITnWOAFOV3qhksjAcxbI3iCGxC7rE1OTtAPTiVsuWtdkEk8'
ACCESS_TOKEN = '1646245338045349888-AnjUoA8yoQ5iThY57LZW0mrsVufQ7P'
ACCESS_TOKEN_SECRET = 'do5rfXGPT5OaMJcoZwDVcfbTQRlqZPDgnq64vlIw8c0lO'
BEARER_TOKEN = 'AAAAAAAAAAAAAAAAAAAAAJEioAEAAAAAfdcUcfrG9UU6i5C8cyunJjvtz44%3DDamiep88WsI58HONgQzU3BQ9asUdvwhtsKBruWdY5QFn7ZtWcc'

# Twitter API endpoint
base_url = "https://api.twitter.com/2/tweets/search/recent"

# Query parameters
params = {
    'query': 'metaverse',
    "tweet.fields": "created_at",
    "max_results": 2,
}

# Create a session object for making requests
session = requests.Session()
session.headers.update({"Authorization": f"Bearer {BEARER_TOKEN}"})

# Function to retrieve tweets using pagination
def retrieve_tweets():
    url = base_url
    response = session.get(url, params=params)
    if response.status_code != 200:
        print(f"Error: {response.status_code} - {response.text}")
        return

    json_response = response.json()
    tweets = json_response.get("data", [])

    for tweet in tweets:
        yield tweet

# Save tweet data to CSV file
def save_tweets_to_csv(tweets, filename):
    tweet_list = list(tweets)
    df = pd.DataFrame(tweet_list)
    df.to_csv(filename, index=False)

# Retrieve and save tweets
tweets_generator = retrieve_tweets()
raw_tweets_path = os.path.join("data", "raw", "tweets_raw.csv")
save_tweets_to_csv(tweets_generator, raw_tweets_path)
